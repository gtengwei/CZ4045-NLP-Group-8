{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fionchai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fionchai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # comment this if you dont have the dev_set\n",
    "\n",
    "# import dataset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "test_set = pd.read_csv('test.csv')\n",
    "\n",
    "# from train_set sample development set\n",
    "dev_set = train_set.sample(n=500, replace=False)\n",
    "\n",
    "# remove dev set from train set\n",
    "train_set = train_set.drop(dev_set.index)\n",
    "\n",
    "# check\n",
    "print(train_set.shape, dev_set.shape, test_set.shape)\n",
    "\n",
    "# save to dataframe\n",
    "dev_set.to_csv(\"dev_set.csv\", index=False)\n",
    "train_set.to_csv(\"train_set_modified.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set_modified.csv')\n",
    "dev_set = pd.read_csv('dev_set.csv')\n",
    "test_set = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data (averaging over word representations)\n",
    "\n",
    "TODO: Try max pooling\n",
    "\n",
    "TODO: Take representation of last word in LSTM\n",
    "\n",
    "TODO: Use attention and perform weighted average?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "\n",
    "# download the word2vec-google-news-300\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select four classes: 0, 1, 2, 3\n",
    "# 4 and 5 will be OTHERS (4)\n",
    "\n",
    "# for train_set\n",
    "train_set.loc[train_set['label-coarse'] > 4, 'label-coarse'] = 4\n",
    "\n",
    "# for dev_set\n",
    "dev_set.loc[dev_set['label-coarse'] > 4, 'label-coarse'] = 4\n",
    "\n",
    "# for test_set\n",
    "test_set.loc[test_set['label-coarse'] > 4, 'label-coarse'] = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network transforming the input for each word to its final vector representation\n",
    "def token(sentence):  \n",
    "      \n",
    "    # keep only english words\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
    "    \n",
    "    # converting to lower case and splitting\n",
    "\n",
    "    # stop word removal\n",
    "    words = sentence.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "\n",
    "    token = word_tokenize(filtered_sentence)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>[serfdom, develop, leave, russia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>[films, featured, character, popeye, doyle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>[find, list, celebrities, real, names]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>[fowl, grabs, spotlight, chinese, year, monkey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>[full, form, com]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label-coarse  label-fine  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             0           0   \n",
       "3             1           2   \n",
       "4             2           3   \n",
       "\n",
       "                                                text  \\\n",
       "0  How did serfdom develop in and then leave Russ...   \n",
       "1   What films featured the character Popeye Doyle ?   \n",
       "2  How can I find a list of celebrities ' real na...   \n",
       "3  What fowl grabs the spotlight after the Chines...   \n",
       "4                    What is the full form of .com ?   \n",
       "\n",
       "                                      cleaned_text  \n",
       "0                [serfdom, develop, leave, russia]  \n",
       "1      [films, featured, character, popeye, doyle]  \n",
       "2           [find, list, celebrities, real, names]  \n",
       "3  [fowl, grabs, spotlight, chinese, year, monkey]  \n",
       "4                                [full, form, com]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['cleaned_text'] = train_set['text'].apply(token)\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>vector</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>[serfdom, develop, leave, russia]</td>\n",
       "      <td>[-0.013671875, -0.05543518, 0.05633545, 0.2869...</td>\n",
       "      <td>-0.013672</td>\n",
       "      <td>-0.055435</td>\n",
       "      <td>0.056335</td>\n",
       "      <td>0.286987</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>-0.066833</td>\n",
       "      <td>0.108032</td>\n",
       "      <td>-0.099915</td>\n",
       "      <td>-0.030396</td>\n",
       "      <td>-0.114136</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>0.092072</td>\n",
       "      <td>0.074524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>[films, featured, character, popeye, doyle]</td>\n",
       "      <td>[-0.00927734375, 0.07685546875, -0.05764770507...</td>\n",
       "      <td>-0.009277</td>\n",
       "      <td>0.076855</td>\n",
       "      <td>-0.057648</td>\n",
       "      <td>0.125146</td>\n",
       "      <td>0.076880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095728</td>\n",
       "      <td>0.041797</td>\n",
       "      <td>-0.101270</td>\n",
       "      <td>-0.003284</td>\n",
       "      <td>0.016504</td>\n",
       "      <td>-0.080591</td>\n",
       "      <td>0.035718</td>\n",
       "      <td>-0.049768</td>\n",
       "      <td>-0.037793</td>\n",
       "      <td>0.058252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>[find, list, celebrities, real, names]</td>\n",
       "      <td>[0.029272461, 0.13002929, -0.021777343, 0.1398...</td>\n",
       "      <td>0.029272</td>\n",
       "      <td>0.130029</td>\n",
       "      <td>-0.021777</td>\n",
       "      <td>0.139868</td>\n",
       "      <td>-0.089648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121045</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>-0.173242</td>\n",
       "      <td>-0.005151</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>-0.103085</td>\n",
       "      <td>0.148145</td>\n",
       "      <td>0.016211</td>\n",
       "      <td>-0.042261</td>\n",
       "      <td>-0.036743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>[fowl, grabs, spotlight, chinese, year, monkey]</td>\n",
       "      <td>[0.06305949, 0.03805542, -0.08516184, 0.015625...</td>\n",
       "      <td>0.063059</td>\n",
       "      <td>0.038055</td>\n",
       "      <td>-0.085162</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.072367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067118</td>\n",
       "      <td>0.119939</td>\n",
       "      <td>-0.064402</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>-0.031352</td>\n",
       "      <td>0.024129</td>\n",
       "      <td>-0.016764</td>\n",
       "      <td>0.100647</td>\n",
       "      <td>-0.012685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>[full, form, com]</td>\n",
       "      <td>[-0.036621094, -0.007965088, -0.08154297, 0.04...</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>-0.007965</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.040934</td>\n",
       "      <td>-0.125732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>-0.029806</td>\n",
       "      <td>-0.025960</td>\n",
       "      <td>-0.069010</td>\n",
       "      <td>-0.023275</td>\n",
       "      <td>0.061747</td>\n",
       "      <td>0.042867</td>\n",
       "      <td>-0.121297</td>\n",
       "      <td>0.025879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label-coarse  label-fine  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             0           0   \n",
       "3             1           2   \n",
       "4             2           3   \n",
       "\n",
       "                                                text  \\\n",
       "0  How did serfdom develop in and then leave Russ...   \n",
       "1   What films featured the character Popeye Doyle ?   \n",
       "2  How can I find a list of celebrities ' real na...   \n",
       "3  What fowl grabs the spotlight after the Chines...   \n",
       "4                    What is the full form of .com ?   \n",
       "\n",
       "                                      cleaned_text  \\\n",
       "0                [serfdom, develop, leave, russia]   \n",
       "1      [films, featured, character, popeye, doyle]   \n",
       "2           [find, list, celebrities, real, names]   \n",
       "3  [fowl, grabs, spotlight, chinese, year, monkey]   \n",
       "4                                [full, form, com]   \n",
       "\n",
       "                                              vector         0         1  \\\n",
       "0  [-0.013671875, -0.05543518, 0.05633545, 0.2869... -0.013672 -0.055435   \n",
       "1  [-0.00927734375, 0.07685546875, -0.05764770507... -0.009277  0.076855   \n",
       "2  [0.029272461, 0.13002929, -0.021777343, 0.1398...  0.029272  0.130029   \n",
       "3  [0.06305949, 0.03805542, -0.08516184, 0.015625...  0.063059  0.038055   \n",
       "4  [-0.036621094, -0.007965088, -0.08154297, 0.04... -0.036621 -0.007965   \n",
       "\n",
       "          2         3         4  ...       290       291       292       293  \\\n",
       "0  0.056335  0.286987 -0.012817  ... -0.249939  0.277100 -0.066833  0.108032   \n",
       "1 -0.057648  0.125146  0.076880  ... -0.095728  0.041797 -0.101270 -0.003284   \n",
       "2 -0.021777  0.139868 -0.089648  ...  0.121045  0.068848 -0.173242 -0.005151   \n",
       "3 -0.085162  0.015625 -0.072367  ... -0.067118  0.119939 -0.064402  0.045369   \n",
       "4 -0.081543  0.040934 -0.125732  ... -0.018311  0.028564 -0.029806 -0.025960   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.099915 -0.030396 -0.114136  0.048099  0.092072  0.074524  \n",
       "1  0.016504 -0.080591  0.035718 -0.049768 -0.037793  0.058252  \n",
       "2  0.018604 -0.103085  0.148145  0.016211 -0.042261 -0.036743  \n",
       "3  0.052500 -0.031352  0.024129 -0.016764  0.100647 -0.012685  \n",
       "4 -0.069010 -0.023275  0.061747  0.042867 -0.121297  0.025879  \n",
       "\n",
       "[5 rows x 305 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = train_set['cleaned_text'].str.len().max()\n",
    "\n",
    "# get the embedding shape of the model\n",
    "embed_shape = len(w2v['test'])\n",
    "average_word_embeddings = []\n",
    "\n",
    "for index, row in train_set.iterrows():\n",
    "\n",
    "    sentence = row['cleaned_text']\n",
    "\n",
    "    # get word embedding of each word\n",
    "    word_embeddings = []\n",
    "\n",
    "    for word in sentence:\n",
    "        # check if the word is present in the model\n",
    "        if word in w2v.key_to_index:\n",
    "            word_embeddings.append(w2v[word])\n",
    "        else:\n",
    "             word_embeddings.append(np.zeros(shape=(embed_shape)))\n",
    "    \n",
    "    # perform averaging of word embeddings\n",
    "    awe = np.mean(word_embeddings, axis = 0)\n",
    "    average_word_embeddings.append(awe)\n",
    "\n",
    "train_set['vector'] = average_word_embeddings\n",
    "\n",
    "train_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>vector</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>[serfdom, develop, leave, russia]</td>\n",
       "      <td>[-0.013671875, -0.05543518, 0.05633545, 0.2869...</td>\n",
       "      <td>-0.013672</td>\n",
       "      <td>-0.055435</td>\n",
       "      <td>0.056335</td>\n",
       "      <td>0.286987</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>-0.066833</td>\n",
       "      <td>0.108032</td>\n",
       "      <td>-0.099915</td>\n",
       "      <td>-0.030396</td>\n",
       "      <td>-0.114136</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>0.092072</td>\n",
       "      <td>0.074524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>[films, featured, character, popeye, doyle]</td>\n",
       "      <td>[-0.00927734375, 0.07685546875, -0.05764770507...</td>\n",
       "      <td>-0.009277</td>\n",
       "      <td>0.076855</td>\n",
       "      <td>-0.057648</td>\n",
       "      <td>0.125146</td>\n",
       "      <td>0.076880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095728</td>\n",
       "      <td>0.041797</td>\n",
       "      <td>-0.101270</td>\n",
       "      <td>-0.003284</td>\n",
       "      <td>0.016504</td>\n",
       "      <td>-0.080591</td>\n",
       "      <td>0.035718</td>\n",
       "      <td>-0.049768</td>\n",
       "      <td>-0.037793</td>\n",
       "      <td>0.058252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>[find, list, celebrities, real, names]</td>\n",
       "      <td>[0.029272461, 0.13002929, -0.021777343, 0.1398...</td>\n",
       "      <td>0.029272</td>\n",
       "      <td>0.130029</td>\n",
       "      <td>-0.021777</td>\n",
       "      <td>0.139868</td>\n",
       "      <td>-0.089648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121045</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>-0.173242</td>\n",
       "      <td>-0.005151</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>-0.103085</td>\n",
       "      <td>0.148145</td>\n",
       "      <td>0.016211</td>\n",
       "      <td>-0.042261</td>\n",
       "      <td>-0.036743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>[fowl, grabs, spotlight, chinese, year, monkey]</td>\n",
       "      <td>[0.06305949, 0.03805542, -0.08516184, 0.015625...</td>\n",
       "      <td>0.063059</td>\n",
       "      <td>0.038055</td>\n",
       "      <td>-0.085162</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.072367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067118</td>\n",
       "      <td>0.119939</td>\n",
       "      <td>-0.064402</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>-0.031352</td>\n",
       "      <td>0.024129</td>\n",
       "      <td>-0.016764</td>\n",
       "      <td>0.100647</td>\n",
       "      <td>-0.012685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>[full, form, com]</td>\n",
       "      <td>[-0.036621094, -0.007965088, -0.08154297, 0.04...</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>-0.007965</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.040934</td>\n",
       "      <td>-0.125732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>-0.029806</td>\n",
       "      <td>-0.025960</td>\n",
       "      <td>-0.069010</td>\n",
       "      <td>-0.023275</td>\n",
       "      <td>0.061747</td>\n",
       "      <td>0.042867</td>\n",
       "      <td>-0.121297</td>\n",
       "      <td>0.025879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>What 's the shape of a camel 's spine ?</td>\n",
       "      <td>[shape, camel, spine]</td>\n",
       "      <td>[-0.0081380205, 0.15018718, -0.16389973, -0.10...</td>\n",
       "      <td>-0.008138</td>\n",
       "      <td>0.150187</td>\n",
       "      <td>-0.163900</td>\n",
       "      <td>-0.106496</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286296</td>\n",
       "      <td>0.071208</td>\n",
       "      <td>-0.016764</td>\n",
       "      <td>-0.040283</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0.173991</td>\n",
       "      <td>-0.025716</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>-0.012207</td>\n",
       "      <td>-0.010235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>What type of currency is used in China ?</td>\n",
       "      <td>[type, currency, used, china]</td>\n",
       "      <td>[-0.08856201, 0.027709961, -0.017578125, 0.199...</td>\n",
       "      <td>-0.088562</td>\n",
       "      <td>0.027710</td>\n",
       "      <td>-0.017578</td>\n",
       "      <td>0.199829</td>\n",
       "      <td>-0.134949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118896</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>-0.041016</td>\n",
       "      <td>0.050514</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>-0.041382</td>\n",
       "      <td>0.116051</td>\n",
       "      <td>0.094116</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>What is the temperature today ?</td>\n",
       "      <td>[temperature, today]</td>\n",
       "      <td>[-0.1574707, -0.010375977, -0.0075683594, -0.0...</td>\n",
       "      <td>-0.157471</td>\n",
       "      <td>-0.010376</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>-0.036072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046936</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>0.133674</td>\n",
       "      <td>0.122314</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>-0.073608</td>\n",
       "      <td>0.133301</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>-0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>What is the temperature for cooking ?</td>\n",
       "      <td>[temperature, cooking]</td>\n",
       "      <td>[-0.19091797, 0.1459961, -0.004272461, 0.10058...</td>\n",
       "      <td>-0.190918</td>\n",
       "      <td>0.145996</td>\n",
       "      <td>-0.004272</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163574</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.179199</td>\n",
       "      <td>0.211914</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>-0.026001</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>0.082275</td>\n",
       "      <td>0.162109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>What currency is used in Australia ?</td>\n",
       "      <td>[currency, used, australia]</td>\n",
       "      <td>[-0.07259115, -0.11539713, -0.14746094, 0.1160...</td>\n",
       "      <td>-0.072591</td>\n",
       "      <td>-0.115397</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>0.116048</td>\n",
       "      <td>-0.100016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>0.219950</td>\n",
       "      <td>0.095052</td>\n",
       "      <td>0.084819</td>\n",
       "      <td>0.020304</td>\n",
       "      <td>0.028931</td>\n",
       "      <td>-0.007161</td>\n",
       "      <td>0.070964</td>\n",
       "      <td>-0.083171</td>\n",
       "      <td>0.006348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4952 rows × 2105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse  label-fine  \\\n",
       "0                0           0   \n",
       "1                1           1   \n",
       "2                0           0   \n",
       "3                1           2   \n",
       "4                2           3   \n",
       "...            ...         ...   \n",
       "4947             1          14   \n",
       "4948             1          46   \n",
       "4949             4          41   \n",
       "4950             4          41   \n",
       "4951             1          46   \n",
       "\n",
       "                                                   text  \\\n",
       "0     How did serfdom develop in and then leave Russ...   \n",
       "1      What films featured the character Popeye Doyle ?   \n",
       "2     How can I find a list of celebrities ' real na...   \n",
       "3     What fowl grabs the spotlight after the Chines...   \n",
       "4                       What is the full form of .com ?   \n",
       "...                                                 ...   \n",
       "4947            What 's the shape of a camel 's spine ?   \n",
       "4948           What type of currency is used in China ?   \n",
       "4949                    What is the temperature today ?   \n",
       "4950              What is the temperature for cooking ?   \n",
       "4951               What currency is used in Australia ?   \n",
       "\n",
       "                                         cleaned_text  \\\n",
       "0                   [serfdom, develop, leave, russia]   \n",
       "1         [films, featured, character, popeye, doyle]   \n",
       "2              [find, list, celebrities, real, names]   \n",
       "3     [fowl, grabs, spotlight, chinese, year, monkey]   \n",
       "4                                   [full, form, com]   \n",
       "...                                               ...   \n",
       "4947                            [shape, camel, spine]   \n",
       "4948                    [type, currency, used, china]   \n",
       "4949                             [temperature, today]   \n",
       "4950                           [temperature, cooking]   \n",
       "4951                      [currency, used, australia]   \n",
       "\n",
       "                                                 vector         0         1  \\\n",
       "0     [-0.013671875, -0.05543518, 0.05633545, 0.2869... -0.013672 -0.055435   \n",
       "1     [-0.00927734375, 0.07685546875, -0.05764770507... -0.009277  0.076855   \n",
       "2     [0.029272461, 0.13002929, -0.021777343, 0.1398...  0.029272  0.130029   \n",
       "3     [0.06305949, 0.03805542, -0.08516184, 0.015625...  0.063059  0.038055   \n",
       "4     [-0.036621094, -0.007965088, -0.08154297, 0.04... -0.036621 -0.007965   \n",
       "...                                                 ...       ...       ...   \n",
       "4947  [-0.0081380205, 0.15018718, -0.16389973, -0.10... -0.008138  0.150187   \n",
       "4948  [-0.08856201, 0.027709961, -0.017578125, 0.199... -0.088562  0.027710   \n",
       "4949  [-0.1574707, -0.010375977, -0.0075683594, -0.0... -0.157471 -0.010376   \n",
       "4950  [-0.19091797, 0.1459961, -0.004272461, 0.10058... -0.190918  0.145996   \n",
       "4951  [-0.07259115, -0.11539713, -0.14746094, 0.1160... -0.072591 -0.115397   \n",
       "\n",
       "             2         3         4  ...       290       291       292  \\\n",
       "0     0.056335  0.286987 -0.012817  ... -0.249939  0.277100 -0.066833   \n",
       "1    -0.057648  0.125146  0.076880  ... -0.095728  0.041797 -0.101270   \n",
       "2    -0.021777  0.139868 -0.089648  ...  0.121045  0.068848 -0.173242   \n",
       "3    -0.085162  0.015625 -0.072367  ... -0.067118  0.119939 -0.064402   \n",
       "4    -0.081543  0.040934 -0.125732  ... -0.018311  0.028564 -0.029806   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4947 -0.163900 -0.106496 -0.166667  ... -0.286296  0.071208 -0.016764   \n",
       "4948 -0.017578  0.199829 -0.134949  ... -0.118896  0.063400 -0.041016   \n",
       "4949 -0.007568 -0.028320 -0.036072  ... -0.046936  0.046387 -0.136719   \n",
       "4950 -0.004272  0.100586  0.017090  ... -0.163574  0.000977 -0.179199   \n",
       "4951 -0.147461  0.116048 -0.100016  ... -0.112305  0.219950  0.095052   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     0.108032 -0.099915 -0.030396 -0.114136  0.048099  0.092072  0.074524  \n",
       "1    -0.003284  0.016504 -0.080591  0.035718 -0.049768 -0.037793  0.058252  \n",
       "2    -0.005151  0.018604 -0.103085  0.148145  0.016211 -0.042261 -0.036743  \n",
       "3     0.045369  0.052500 -0.031352  0.024129 -0.016764  0.100647 -0.012685  \n",
       "4    -0.025960 -0.069010 -0.023275  0.061747  0.042867 -0.121297  0.025879  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4947 -0.040283 -0.005208  0.173991 -0.025716  0.273600 -0.012207 -0.010235  \n",
       "4948  0.050514  0.052826 -0.041382  0.116051  0.094116  0.016174  0.000977  \n",
       "4949  0.133674  0.122314 -0.099609 -0.073608  0.133301  0.076477 -0.032227  \n",
       "4950  0.211914  0.146484 -0.132812 -0.026001  0.057129  0.082275  0.162109  \n",
       "4951  0.084819  0.020304  0.028931 -0.007161  0.070964 -0.083171  0.006348  \n",
       "\n",
       "[4952 rows x 2105 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate_representations(dataset):\n",
    "  dataset['cleaned_text'] = dataset['text'].apply(token)\n",
    "  max_length = dataset['cleaned_text'].str.len().max()\n",
    "\n",
    "  # get the embedding shape of the model\n",
    "  embed_shape = len(w2v['test'])\n",
    "  average_word_embeddings = []\n",
    "\n",
    "  for index, row in dataset.iterrows():\n",
    "\n",
    "      sentence = row['cleaned_text']\n",
    "\n",
    "      # get word embedding of each word\n",
    "      word_embeddings = []\n",
    "\n",
    "      for word in sentence:\n",
    "          # check if the word is present in the model\n",
    "          if word in w2v.key_to_index:\n",
    "              word_embeddings.append(w2v[word])\n",
    "          else:\n",
    "              word_embeddings.append(np.zeros(shape=(embed_shape)))\n",
    "      \n",
    "      # perform averaging of word embeddings\n",
    "      awe = np.mean(word_embeddings, axis = 0)\n",
    "\n",
    "      average_word_embeddings.append(awe)\n",
    "\n",
    "  dataset['vector'] = average_word_embeddings\n",
    "\n",
    "  dataset = pd.concat([train_set, train_set['vector'].apply(lambda x: pd.Series(x))], axis=1)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "dev_set = aggregate_representations(dev_set)\n",
    "train_set = aggregate_representations(train_set)\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y\n",
    "X_train = train_set.drop(columns=['label-coarse', 'label-fine', 'text', 'cleaned_text', 'vector']).to_numpy()\n",
    "y_train = train_set['label-coarse']\n",
    "\n",
    "X_dev = dev_set.drop(columns=['label-coarse', 'label-fine', 'text', 'cleaned_text', 'vector']).to_numpy()\n",
    "y_dev = dev_set['label-coarse']\n",
    "\n",
    "# SCALING?\n",
    "\n",
    "# mini-batch strategy\n",
    "batch_size = 32\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     x, y = data\n",
    "#     zipped = zip(x, y)\n",
    "#     return list(zipped)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X =torch.tensor(X, dtype=torch.float64)\n",
    "        self.y =torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_data = CustomDataset(X_train, y_train)\n",
    "dev_data = CustomDataset(X_dev, y_dev)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed into the softmax classifier to predict the final label\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model \n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_d, output_d),\n",
    "            nn.Softmax(dim= 1) # softmax activation for label prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.reshape(32, 7, 300).float() # convert to 3D tensor\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.head(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "input_dim = embed_shape\n",
    "hidden_dim = 128\n",
    "output_dim = 5\n",
    "layer_dim = 300\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, training set loss: nan\n",
      "0 nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/fionchai/Documents/GitHub/CZ4045-Group-8/Part 2/Part_2.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fionchai/Documents/GitHub/CZ4045-Group-8/Part%202/Part_2.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(x_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fionchai/Documents/GitHub/CZ4045-Group-8/Part%202/Part_2.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fionchai/Documents/GitHub/CZ4045-Group-8/Part%202/Part_2.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fionchai/Documents/GitHub/CZ4045-Group-8/Part%202/Part_2.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fionchai/Documents/GitHub/CZ4045-Group-8/Part%202/Part_2.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m total_dev_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# REFERENCE\n",
    "num_epochs = 100\n",
    "best_accuracy = 0.0\n",
    "max_patience = 5\n",
    "current_patience = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  total_training_loss = 0.0\n",
    "  total_dev_loss = 0.0\n",
    "  \n",
    "  for x_batch, y_batch in train_loader:\n",
    "    outputs = model.forward(x_batch) # forward pass\n",
    "    optimizer.zero_grad() # calculate the gradient, manually setting to 0\n",
    "  \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y_batch)\n",
    "  \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "  \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "\n",
    "    total_training_loss += loss.item()\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    average_loss = total_training_loss / len(train_loader)\n",
    "    \n",
    "    print(\"Epoch: %d, training set loss: %1.5f\" % (epoch, average_loss))\n",
    "\n",
    "  # evaluate on dev set\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for inputs in dev_loader:\n",
    "      x_batch, y_batch = inputs\n",
    "\n",
    "      # forward pass\n",
    "      y_pred = model.forward(x_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_dev_loss += loss.item()\n",
    "      if epoch % 100 == 0:\n",
    "        average_dev_loss = total_dev_loss / len(dev_loader)\n",
    "        print(\"Epoch: %d, dev set loss: %1.5f\" % (epoch, average_dev_loss))\n",
    "\n",
    "      predicted_labels = torch.argmax(y_pred, dim=1)\n",
    "      \n",
    "      all_true_labels = list(y_batch)\n",
    "      all_predicted_labels = list(predicted_labels)\n",
    "              \n",
    "  def calculate_accuracy(y_true, y_pred):\n",
    "      y_true = y_true.detach().cpu().numpy()\n",
    "      y_pred = y_pred.detach().cpu().numpy()\n",
    "      return accuracy_score(y_true, y_pred)\n",
    "\n",
    "  accuracy = calculate_accuracy(torch.tensor(all_true_labels), torch.tensor(all_predicted_labels))\n",
    "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "  if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_model = model.state_dict()\n",
    "  else:\n",
    "    current_patience += 1\n",
    "    if current_patience >= max_patience:\n",
    "        print(f\"Early stopping after {epoch} epochs.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
