{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # comment this if you dont have teh dev_set\n",
    "\n",
    "# import dataset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "test_set = pd.read_csv('test.csv')\n",
    "\n",
    "# from train_set sample development set\n",
    "dev_set = train_set.sample(n=500, replace=False)\n",
    "\n",
    "# remove dev set from train set\n",
    "train_set = train_set.drop(dev_set.index)\n",
    "\n",
    "# check\n",
    "print(train_set.shape, dev_set.shape, test_set.shape)\n",
    "\n",
    "# save to dataframe\n",
    "dev_set.to_csv(\"dev_set.csv\", index=False)\n",
    "train_set.to_csv(\"train_set_modified.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set_modified.csv')\n",
    "dev_set = pd.read_csv('dev_set.csv')\n",
    "test_set = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data (averaging over word representations)\n",
    "\n",
    "TODO: Try max pooling\n",
    "\n",
    "TODO: Take representation of last word in LSTM\n",
    "\n",
    "TODO: Use attention and perform weighted average?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "\n",
    "# download the word2vec-google-news-300\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select four classes: 0, 1, 2, 3\n",
    "# 4 and 5 will be OTHERS (4)\n",
    "\n",
    "# for train_set\n",
    "train_set.loc[train_set['label-coarse'] > 4, 'label-coarse'] = 4\n",
    "\n",
    "# for dev_set\n",
    "dev_set.loc[dev_set['label-coarse'] > 4, 'label-coarse'] = 4\n",
    "\n",
    "# for test_set\n",
    "test_set.loc[test_set['label-coarse'] > 4, 'label-coarse'] = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network transforming the input for each word to its final vector representation\n",
    "def token(sentence):  \n",
    "      \n",
    "    # keep only english words\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
    "    \n",
    "    # converting to lower case and splitting\n",
    "\n",
    "    # stop word removal\n",
    "    words = sentence.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "\n",
    "    token = word_tokenize(filtered_sentence)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['cleaned_text'] = train_set['text'].apply(token)\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = train_set['cleaned_text'].str.len().max()\n",
    "\n",
    "# get the embedding shape of the model\n",
    "embed_shape = len(w2v['test'])\n",
    "average_word_embeddings = []\n",
    "\n",
    "for index, row in train_set.iterrows():\n",
    "\n",
    "    sentence = row['cleaned_text']\n",
    "\n",
    "    # get word embedding of each word\n",
    "    word_embeddings = []\n",
    "\n",
    "    for word in sentence:\n",
    "        # check if the word is present in the model\n",
    "        if word in w2v.key_to_index:\n",
    "            word_embeddings.append(w2v[word])\n",
    "        else:\n",
    "             word_embeddings.append(np.zeros(shape=(embed_shape)))\n",
    "    \n",
    "    # perform averaging of word embeddings\n",
    "    awe = np.mean(word_embeddings, axis = 0)\n",
    "    average_word_embeddings.append(awe)\n",
    "\n",
    "train_set['vector'] = average_word_embeddings\n",
    "\n",
    "train_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_representations(dataset):\n",
    "  dataset['cleaned_text'] = dataset['text'].apply(token)\n",
    "  max_length = dataset['cleaned_text'].str.len().max()\n",
    "\n",
    "  # get the embedding shape of the model\n",
    "  embed_shape = len(w2v['test'])\n",
    "  average_word_embeddings = []\n",
    "\n",
    "  for index, row in dataset.iterrows():\n",
    "\n",
    "      sentence = row['cleaned_text']\n",
    "\n",
    "      # get word embedding of each word\n",
    "      word_embeddings = []\n",
    "\n",
    "      for word in sentence:\n",
    "          # check if the word is present in the model\n",
    "          if word in w2v.key_to_index:\n",
    "              word_embeddings.append(w2v[word])\n",
    "          else:\n",
    "              word_embeddings.append(np.zeros(shape=(embed_shape)))\n",
    "      \n",
    "      # perform averaging of word embeddings\n",
    "      awe = np.mean(word_embeddings, axis = 0)\n",
    "\n",
    "      average_word_embeddings.append(awe)\n",
    "\n",
    "  dataset['vector'] = average_word_embeddings\n",
    "  return dataset\n",
    "\n",
    "dev_set = aggregate_representations(dev_set)\n",
    "train_set = aggregate_representations(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y\n",
    "X_train = train_set['vector']\n",
    "y_train = train_set['label-coarse']\n",
    "\n",
    "X_dev = dev_set['vector']\n",
    "y_dev = dev_set['label-coarse']\n",
    "\n",
    "# SCALING?\n",
    "\n",
    "# mini-batch strategy\n",
    "batch_size = 32\n",
    "\n",
    "def collate_fn(data):\n",
    "    x, y = data\n",
    "    zipped = zip(x, y)\n",
    "    return list(zipped)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed into the softmax classifier to predict the final label\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model \n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_d, output_d),\n",
    "            nn.Softmax(dim= 1) # softmax activation for label prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(32, 1, 300).float() # convert to 3D tensor\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.head(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "input_dim = embed_shape\n",
    "hidden_dim = 128\n",
    "output_dim = 5\n",
    "layer_dim = 1\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE\n",
    "num_epochs = 100\n",
    "best_accuracy = 0.0\n",
    "max_patience = 5\n",
    "current_patience = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  total_training_loss = 0.0\n",
    "  total_dev_loss = 0.0\n",
    "\n",
    "  for x_batch, y_batch in train_loader:\n",
    "    outputs = model.forward(x_batch) # forward pass\n",
    "    optimizer.zero_grad() # calculate the gradient, manually setting to 0\n",
    "  \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y_batch)\n",
    "  \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "  \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "\n",
    "    total_training_loss += loss.item()\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    average_loss = total_training_loss / len(train_loader)\n",
    "    print(\"Epoch: %d, training set loss: %1.5f\" % (epoch, average_loss))\n",
    "\n",
    "  # evaluate on dev set\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for inputs in dev_loader:\n",
    "      x_batch, y_batch = inputs\n",
    "\n",
    "      # forward pass\n",
    "      y_pred = model.forward(x_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_dev_loss += loss.item()\n",
    "      if epoch % 100 == 0:\n",
    "        average_dev_loss = total_dev_loss / len(dev_loader)\n",
    "        print(\"Epoch: %d, dev set loss: %1.5f\" % (epoch, average_dev_loss))\n",
    "\n",
    "      predicted_labels = torch.argmax(y_pred, dim=1)\n",
    "      \n",
    "      all_true_labels = list(y_batch)\n",
    "      all_predicted_labels = list(predicted_labels)\n",
    "              \n",
    "  def calculate_accuracy(y_true, y_pred):\n",
    "      y_true = y_true.detach().cpu().numpy()\n",
    "      y_pred = y_pred.detach().cpu().numpy()\n",
    "      return accuracy_score(y_true, y_pred)\n",
    "\n",
    "  accuracy = calculate_accuracy(torch.tensor(all_true_labels), torch.tensor(all_predicted_labels))\n",
    "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "  if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_model = model.state_dict()\n",
    "  else:\n",
    "    current_patience += 1\n",
    "    if current_patience >= max_patience:\n",
    "        print(f\"Early stopping after {epoch} epochs.\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
